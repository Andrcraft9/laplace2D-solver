MPI init is ok. procs = 1 threads = 1
npx = 1 npy = 1 locM = 2048 locN = 2048
Laplace Solver, MPI/CUDA
M = 2048 N = 2048 maxiters = 1000 tol = 1
Version with profiling
MRM: iters: 1001 residual: 693.395
MRM: time sync: 0.00012977 time sync (max): 0.00012977
MRM: time matvec: 0.0139033 time matvec (max): 0.0139033
MRM: time dot: 7.6015 time dot (max): 7.6015
MRM: time axpy: 0.0105275 time axpy (max): 0.0105275
MRM: iters: 1001 resudial: 693.395
MRM: Warning! Max Iterations in MRM solver!
Time: 7.68341
Error (L2): 2879.68
Error (C): 2
Cores & Threads per core &   Mesh  & Time (sec) & Iterations & Error (L2) & Error (C) 
1 & 1 & 2048 x 2048 & 7.683407 & 1001 & 2879.683993 & 2.000000 

COMPARE WITH PURE MPI:

MPI init is ok. procs = 1 threads = 1
npx = 1 npy = 1 locM = 2048 locN = 2048
Laplace Solver, pure mpi
M = 2048 N = 2048 maxiters = 1000 tol = 1 profile = 1
MRM: iters: 1001 residual: 693.395
MRM: time sync: 0.105429 time sync (max): 0.105429
MRM: time matvec: 9.40912 time matvec (max): 9.40912
MRM: time norm: 14.4424 time norm (max): 14.4424
MRM: time axpy: 8.63306 time axpy (max): 8.63306
MRM: Warning! Max Iterations in MRM solver!
Time of solver: 32.6122
Time of solver (max): 32.6122
Error (L2): 2879.68
Error (C): 2
Cores & Threads per core &   Mesh  & Time (sec) & Iterations & Error (L2) & Error (C) 
1 & 1 & 2048 x 2048 & 32.612155 & 1001 & 2879.683993 & 2.000000 

